(carnd-term1) said:lenet_3data$ python model.py -d '../Data/*' -m lenet
Using TensorFlow backend.
Training data will be taken from:  ../Data/*
Output file is  model_lenet  +.h5
Please confirm with "Y"!  :: Y
3 directory as input data found.
../Data/data
../Data/custom2
../Data/custom1
WARNING: Logging before flag parsing goes to stderr.
W0806 23:42:10.542855 139786601727808 deprecation_wrapper.py:119] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0806 23:42:10.552883 139786601727808 deprecation_wrapper.py:119] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0806 23:42:10.561522 139786601727808 deprecation_wrapper.py:119] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

W0806 23:42:10.565900 139786601727808 deprecation.py:506] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0806 23:42:10.575264 139786601727808 deprecation_wrapper.py:119] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0806 23:42:10.583909 139786601727808 deprecation_wrapper.py:119] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
cropping2d_1 (Cropping2D)    (None, 50, 280, 3)        0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 50, 280, 3)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 50, 280, 3)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 48, 278, 6)        168       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 24, 139, 6)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 22, 137, 16)       880       
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 11, 68, 16)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 11968)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 11968)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 120)               1436280   
_________________________________________________________________
batch_normalization_1 (Batch (None, 120)               480       
_________________________________________________________________
dense_2 (Dense)              (None, 84)                10164     
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 85        
=================================================================
Total params: 1,448,057
Trainable params: 1,447,817
Non-trainable params: 240
_________________________________________________________________
Start training with data from directory:  ../Data/data
Start training with data from directory:  ../Data/custom2
Start training with data from directory:  ../Data/custom1
W0806 23:42:10.901411 139786601727808 deprecation_wrapper.py:119] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0806 23:42:11.088018 139786601727808 deprecation_wrapper.py:119] From /home/said/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

Epoch 1/10
2019-08-06 23:42:11.361270: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-06 23:42:11.382202: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz
2019-08-06 23:42:11.382543: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x64a6c40 executing computations on platform Host. Devices:
2019-08-06 23:42:11.382561: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-06 23:42:11.580508: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
1375/1375 [==============================] - 168s 122ms/step - loss: 0.0466 - val_loss: 0.0259

Epoch 00001: saving model to .//checkpoints-8-6-23-0001-01-01 00:00:00-10/model.ckpt
Epoch 2/10
1375/1375 [==============================] - 161s 117ms/step - loss: 0.0313 - val_loss: 0.0254

Epoch 00002: saving model to .//checkpoints-8-6-23-0001-01-01 00:00:00-10/model.ckpt
Epoch 3/10
1375/1375 [==============================] - 164s 119ms/step - loss: 0.0286 - val_loss: 0.0246

Epoch 00003: saving model to .//checkpoints-8-6-23-0001-01-01 00:00:00-10/model.ckpt
Epoch 4/10
1375/1375 [==============================] - 163s 119ms/step - loss: 0.0272 - val_loss: 0.0232

Epoch 00004: saving model to .//checkpoints-8-6-23-0001-01-01 00:00:00-10/model.ckpt
Epoch 5/10
1375/1375 [==============================] - 163s 118ms/step - loss: 0.0260 - val_loss: 0.0235

Epoch 00005: saving model to .//checkpoints-8-6-23-0001-01-01 00:00:00-10/model.ckpt
Epoch 6/10
1375/1375 [==============================] - 164s 119ms/step - loss: 0.0251 - val_loss: 0.0254

Epoch 00006: saving model to .//checkpoints-8-6-23-0001-01-01 00:00:00-10/model.ckpt
Epoch 00006: early stopping
dict_keys(['val_loss', 'loss'])
